{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lante\\miniconda3\\envs\\ai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'num_epochs': 20,\n",
       " 'lr': 0.0001,\n",
       " 'seq_len': 350,\n",
       " 'd_model': 512,\n",
       " 'lang_src': 'en',\n",
       " 'lang_tgt': 'it',\n",
       " 'model_folder': 'weights',\n",
       " 'model_basename': 'tmodel_',\n",
       " 'preload': False,\n",
       " 'param_sharing': True,\n",
       " 'anneal_strategy': 'linear',\n",
       " 'three_phase': True,\n",
       " 'gradient_accumulation': True,\n",
       " 'accumulation_steps': 4,\n",
       " 'h': 8,\n",
       " 'tokenizer_file': 'tokenizer_{0}.json',\n",
       " 'experiment_name': 'runs/tmodel'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from config_file import get_config\n",
    "import torch\n",
    "import train\n",
    "cfg = get_config()\n",
    "\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg['batch_size'] = 32\n",
    "cfg['preload'] = False\n",
    "cfg['num_epochs'] = 30\n",
    "cfg['d_model'] = 256\n",
    "cfg['d_ff'] = 64\n",
    "cfg['pct_start'] = 0.2\n",
    "cfg['max_lr'] = 1e-3\n",
    "cfg['initial_div_factor'] = 10\n",
    "cfg['final_div_factor'] = 100\n",
    "cfg['enable_amp'] = True\n",
    "cfg['gradient_accumulation'] = False\n",
    "cfg['gradient_accumulation_steps'] = 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'num_epochs': 30,\n",
       " 'lr': 0.0001,\n",
       " 'seq_len': 350,\n",
       " 'd_model': 256,\n",
       " 'lang_src': 'en',\n",
       " 'lang_tgt': 'it',\n",
       " 'model_folder': 'weights',\n",
       " 'model_basename': 'tmodel_',\n",
       " 'preload': False,\n",
       " 'param_sharing': True,\n",
       " 'anneal_strategy': 'linear',\n",
       " 'three_phase': True,\n",
       " 'gradient_accumulation': False,\n",
       " 'accumulation_steps': 4,\n",
       " 'h': 8,\n",
       " 'tokenizer_file': 'tokenizer_{0}.json',\n",
       " 'experiment_name': 'runs/tmodel',\n",
       " 'd_ff': 64,\n",
       " 'pct_start': 0.2,\n",
       " 'max_lr': 0.001,\n",
       " 'initial_div_factor': 10,\n",
       " 'final_div_factor': 100,\n",
       " 'enable_amp': True,\n",
       " 'gradient_accumulation_steps': 40}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘€Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch: 00:   0%|          | 0/910 [00:00<?, ?it/s]c:\\Users\\lante\\OneDrive\\Documents\\erav2\\s17\\model.py:146: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  x = F.scaled_dot_product_attention(query=query, key=key, value=value, attn_mask=mask.bool(), dropout_p=0.1)\n",
      "Processing Epoch: 00: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:58<00:00,  5.10it/s, loss=6.48316, lr=[0.0002500274775599927]] \n",
      "Processing Epoch: 01: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:53<00:00,  5.23it/s, loss=5.72773, lr=[0.00040005495511998533]]\n",
      "Processing Epoch: 02: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:53<00:00,  5.24it/s, loss=5.59396, lr=[0.000550082432679978]]  \n",
      "Processing Epoch: 03: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:53<00:00,  5.24it/s, loss=5.14927, lr=[0.0007001099102399707]]\n",
      "Processing Epoch: 04: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:52<00:00,  5.28it/s, loss=5.77962, lr=[0.0008501373877999634]]\n",
      "Processing Epoch: 05: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:53<00:00,  5.24it/s, loss=5.07772, lr=[0.000999835134640044]] \n",
      "Processing Epoch: 06: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:52<00:00,  5.26it/s, loss=4.96186, lr=[0.0008498076570800513]]\n",
      "Processing Epoch: 07: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:53<00:00,  5.24it/s, loss=4.75488, lr=[0.0006997801795200586]]\n",
      "Processing Epoch: 08: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:53<00:00,  5.24it/s, loss=4.49682, lr=[0.000549752701960066]] \n",
      "Processing Epoch: 09: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:53<00:00,  5.25it/s, loss=4.55032, lr=[0.00039972522440007326]]\n",
      "Processing Epoch: 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:53<00:00,  5.24it/s, loss=4.12062, lr=[0.00024969774684008056]]\n",
      "Processing Epoch: 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:52<00:00,  5.29it/s, loss=3.68900, lr=[9.998791282583481e-05]] \n",
      "Processing Epoch: 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:52<00:00,  5.27it/s, loss=4.11301, lr=[9.448824858067274e-05]]\n",
      "Processing Epoch: 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:53<00:00,  5.26it/s, loss=3.89515, lr=[8.898858433551065e-05]]\n",
      "Processing Epoch: 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:53<00:00,  5.25it/s, loss=4.16545, lr=[8.348892009034858e-05]]\n",
      "Processing Epoch: 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:53<00:00,  5.25it/s, loss=3.69117, lr=[7.79892558451865e-05]] \n",
      "Processing Epoch: 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:53<00:00,  5.24it/s, loss=4.31343, lr=[7.248959160002441e-05]]\n",
      "Processing Epoch: 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:55<00:00,  5.18it/s, loss=3.82632, lr=[6.698992735486235e-05]]\n",
      "Processing Epoch: 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:54<00:00,  5.23it/s, loss=3.86438, lr=[6.149026310970027e-05]]\n",
      "Processing Epoch: 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:53<00:00,  5.24it/s, loss=3.71636, lr=[5.5990598864538184e-05]]\n",
      "Processing Epoch: 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:55<00:00,  5.18it/s, loss=3.84734, lr=[5.0490934619376106e-05]]\n",
      "Processing Epoch: 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:54<00:00,  5.21it/s, loss=3.74799, lr=[4.499127037421403e-05]] \n",
      "Processing Epoch: 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:55<00:00,  5.19it/s, loss=3.62663, lr=[3.9491606129051944e-05]]\n",
      "Processing Epoch: 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:53<00:00,  5.24it/s, loss=3.47919, lr=[3.399194188388987e-05]] \n",
      "Processing Epoch: 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:53<00:00,  5.24it/s, loss=3.65510, lr=[2.84922776387278e-05]]  \n",
      "Processing Epoch: 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:51<00:00,  5.29it/s, loss=3.53366, lr=[2.2992613393565717e-05]]\n",
      "Processing Epoch: 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:51<00:00,  5.30it/s, loss=3.63626, lr=[1.7492949148403632e-05]]\n",
      "Processing Epoch: 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:53<00:00,  5.25it/s, loss=3.67641, lr=[1.1993284903241561e-05]]\n",
      "Processing Epoch: 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:53<00:00,  5.24it/s, loss=3.84696, lr=[6.493620658079476e-06]] \n",
      "Processing Epoch: 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:53<00:00,  5.25it/s, loss=3.50526, lr=[9.939564129173917e-07]] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Cold veal pie, when you don't feel hungry, is apt to cloy.\n",
      "    TARGET: Il pasticcio di vitello freddo, quando non si ha fame, puÃ² nauseare.\n",
      " PREDICTED: â€” Non vi agitate , che si la terra il pasticcio â€” disse il pasticcio â€” non vi Ã¨ il pasticcio !\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: He took up his bow and arrows, and came back; so I turned to go away, and beckoned him to follow me, making signs to him that more might come after them.\n",
      "    TARGET: Non credendo io cosa opportuna il rimaner piÃ¹ in quel luogo, gli feci segno di seguitarmi non senza studiarmi di dargli a comprendere, sempre a cenni, come gli altri selvaggi potessero venire dietro a quelli che erano morti.\n",
      " PREDICTED: Egli prese a caricare le VenerdÃ¬ e VenerdÃ¬ e gli dissi che gli veniva loro , , gli dissi che il suo agio e , gli stesi VenerdÃ¬ , gli dissi loro addosso , poi che cosa gli diede un â€™ ora il suo bagaglio .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from train import train_model\n",
    "\n",
    "train_model(config=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'num_epochs': 20,\n",
       " 'lr': 0.0001,\n",
       " 'seq_len': 350,\n",
       " 'd_model': 512,\n",
       " 'lang_src': 'en',\n",
       " 'lang_tgt': 'it',\n",
       " 'model_folder': 'weights',\n",
       " 'model_basename': 'tmodel_',\n",
       " 'preload': False,\n",
       " 'param_sharing': True,\n",
       " 'anneal_strategy': 'linear',\n",
       " 'three_phase': True,\n",
       " 'gradient_accumulation': True,\n",
       " 'accumulation_steps': 4,\n",
       " 'h': 8,\n",
       " 'tokenizer_file': 'tokenizer_{0}.json',\n",
       " 'experiment_name': 'runs/tmodel'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from config_file import get_config\n",
    "import torch\n",
    "import train\n",
    "cfg = get_config()\n",
    "\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg['batch_size'] = 32\n",
    "cfg['preload'] = False\n",
    "cfg['num_epochs'] = 50\n",
    "cfg['d_model'] = 256\n",
    "cfg['d_ff'] = 64\n",
    "cfg['pct_start'] = 0.2\n",
    "cfg['max_lr'] = 1e-2\n",
    "cfg['initial_div_factor'] = 10\n",
    "cfg['final_div_factor'] = 500\n",
    "cfg['enable_amp'] = True\n",
    "cfg['gradient_accumulation'] = True\n",
    "cfg['gradient_accumulation_steps'] = 40\n",
    "cfg['preload'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'num_epochs': 50,\n",
       " 'lr': 0.0001,\n",
       " 'seq_len': 350,\n",
       " 'd_model': 256,\n",
       " 'lang_src': 'en',\n",
       " 'lang_tgt': 'it',\n",
       " 'model_folder': 'weights',\n",
       " 'model_basename': 'tmodel_',\n",
       " 'preload': True,\n",
       " 'param_sharing': True,\n",
       " 'anneal_strategy': 'linear',\n",
       " 'three_phase': True,\n",
       " 'gradient_accumulation': True,\n",
       " 'accumulation_steps': 4,\n",
       " 'h': 8,\n",
       " 'tokenizer_file': 'tokenizer_{0}.json',\n",
       " 'experiment_name': 'runs/tmodel',\n",
       " 'd_ff': 64,\n",
       " 'pct_start': 0.2,\n",
       " 'max_lr': 0.01,\n",
       " 'initial_div_factor': 10,\n",
       " 'final_div_factor': 500,\n",
       " 'enable_amp': True,\n",
       " 'gradient_accumulation_steps': 40}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘€Using device: cuda\n",
      "['weights\\\\tmodel_29.pt', 'weights\\\\tmodel_28.pt', 'weights\\\\tmodel_27.pt', 'weights\\\\tmodel_26.pt', 'weights\\\\tmodel_25.pt', 'weights\\\\tmodel_24.pt', 'weights\\\\tmodel_23.pt', 'weights\\\\tmodel_22.pt', 'weights\\\\tmodel_21.pt', 'weights\\\\tmodel_20.pt', 'weights\\\\tmodel_19.pt', 'weights\\\\tmodel_18.pt', 'weights\\\\tmodel_17.pt', 'weights\\\\tmodel_16.pt', 'weights\\\\tmodel_15.pt', 'weights\\\\tmodel_14.pt', 'weights\\\\tmodel_13.pt', 'weights\\\\tmodel_12.pt', 'weights\\\\tmodel_11.pt', 'weights\\\\tmodel_10.pt', 'weights\\\\tmodel_09.pt', 'weights\\\\tmodel_08.pt', 'weights\\\\tmodel_07.pt', 'weights\\\\tmodel_06.pt', 'weights\\\\tmodel_05.pt', 'weights\\\\tmodel_04.pt', 'weights\\\\tmodel_03.pt', 'weights\\\\tmodel_02.pt', 'weights\\\\tmodel_01.pt', 'weights\\\\tmodel_00.pt']\n",
      "Preloading model weights\\tmodel_29.pt\n",
      "Preloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch: 30:   0%|          | 0/910 [00:00<?, ?it/s]c:\\Users\\lante\\OneDrive\\Documents\\erav2\\s17\\model.py:146: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  x = F.scaled_dot_product_attention(query=query, key=key, value=value, attn_mask=mask.bool(), dropout_p=0.1)\n",
      "Processing Epoch: 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [03:22<00:00,  4.50it/s, Accumulated scaled loss=3.44986, lr=[0.00019000989119683482]]\n",
      "Processing Epoch: 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:50<00:00,  5.34it/s, Accumulated scaled loss=3.72510, lr=[0.00028001978239366966]]\n",
      "Processing Epoch: 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 910/910 [02:50<00:00,  5.33it/s, Accumulated scaled loss=3.25243, lr=[0.00037002967359050446]]\n",
      "Processing Epoch: 33:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 678/910 [02:07<00:43,  5.31it/s, Accumulated scaled loss=3.81111, lr=[0.0004370919881305638]] "
     ]
    }
   ],
   "source": [
    "from train import train_model\n",
    "\n",
    "train_model(config=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
