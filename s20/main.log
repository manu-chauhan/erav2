b'fetch: 33 objects found, done.\nfetch: Fetching all references...\n'
b'Checking out LFS objects: 100% (33/33), 9.6 GB | 0 B/s, done.\n'

=========
Reading dataset
=============

Count of files to read...30


Batch number=====================================================: 0


====================================

Building initial Hindi vocabulary with basic Hindi letters and key tokens

=================
Vocab initialisation done...

`Training`...for HindiTokenizer


merge 1/300: (224, 164) -> 358 (b'\xe0\xa4') had 5_899_449 occurrences


merge 2/300: (32, 358) -> 359 (b' \xe0\xa4') had 1_869_612 occurrences


merge 3/300: (224, 165) -> 360 (b'\xe0\xa5') had 1_684_235 occurrences


merge 4/300: (358, 190) -> 361 (b'\xe0\xa4\xbe') had 672_066 occurrences
b'fetch: 33 objects found, done.\nfetch: Fetching all references...\n'
b'Checking out LFS objects: 100% (33/33), 9.6 GB | 0 B/s, done.\n'

=========
Reading dataset
=============

Count of files to read...30


Batch number=====================================================: 0


====================================

Building initial Hindi vocabulary with basic Hindi letters and key tokens

=================
Vocab initialisation done...

`Training`...for HindiTokenizer
b'fetch: 33 objects found, done.\nfetch: Fetching all references...\n'
b'Checking out LFS objects: 100% (33/33), 9.6 GB | 0 B/s, done.\n'

=========
Reading dataset
=============

Count of files to read...30


Batch number=====================================================: 0


====================================

Building initial Hindi vocabulary with basic Hindi letters and key tokens

=================
Vocab initialisation done...

`Training`...for HindiTokenizer


merge 1/300: (224, 164) -> 358 (b'\xe0\xa4') had 5_899_449 occurrences


merge 2/300: (32, 358) -> 359 (b' \xe0\xa4') had 1_869_612 occurrences


merge 3/300: (224, 165) -> 360 (b'\xe0\xa5') had 1_684_235 occurrences


merge 4/300: (358, 190) -> 361 (b'\xe0\xa4\xbe') had 672_066 occurrences


merge 5/300: (360, 135) -> 362 (b'\xe0\xa5\x87') had 573_180 occurrences


merge 6/300: (358, 176) -> 363 (b'\xe0\xa4\xb0') had 429_162 occurrences


merge 7/300: (358, 130) -> 364 (b'\xe0\xa4\x82') had 373_179 occurrences


merge 8/300: (359, 149) -> 365 (b' \xe0\xa4\x95') had 316_576 occurrences


merge 9/300: (360, 128) -> 366 (b'\xe0\xa5\x80') had 314_253 occurrences


merge 10/300: (361, 358) -> 367 (b'\xe0\xa4\xbe\xe0\xa4') had 297_294 occurrences


merge 11/300: (360, 141) -> 368 (b'\xe0\xa5\x8d') had 280_817 occurrences


merge 12/300: (358, 191) -> 369 (b'\xe0\xa4\xbf') had 253_000 occurrences


merge 13/300: (369, 358) -> 370 (b'\xe0\xa4\xbf\xe0\xa4') had 234_708 occurrences


merge 14/300: (360, 139) -> 371 (b'\xe0\xa5\x8b') had 224_209 occurrences


merge 15/300: (368, 358) -> 372 (b'\xe0\xa5\x8d\xe0\xa4') had 217_356 occurrences


merge 16/300: (358, 168) -> 373 (b'\xe0\xa4\xa8') had 183_326 occurrences
b'fetch: 33 objects found, done.\nfetch: Fetching all references...\n'
b'Checking out LFS objects: 100% (33/33), 9.6 GB | 0 B/s, done.\n'

=========
Reading dataset
=============

Count of files to read...30


Batch number=====================================================: 0


====================================

Building initial Hindi vocabulary with basic Hindi letters and key tokens

=================
Vocab initialisation done...

`Training`...for HindiTokenizer
b'fetch: 33 objects found, done.\nfetch: Fetching all references...\n'
b'Checking out LFS objects: 100% (33/33), 9.6 GB | 0 B/s, done.\n'

=========
Reading dataset
=============

Count of files to read...30


Batch number=====================================================: 0


====================================

Building initial Hindi vocabulary with basic Hindi letters and key tokens

=================
Vocab initialisation done...

`Training`...for HindiTokenizer
b'fetch: 33 objects found, done.\nfetch: Fetching all references...\n'
b'fetch: 33 objects found, done.\nfetch: Fetching all references...\n'
b'Checking out LFS objects: 100% (33/33), 9.6 GB | 0 B/s, done.\n'

=========
Reading dataset
=============

Count of files to read...30


Batch number=====================================================: 0


====================================

Building initial Hindi vocabulary with basic Hindi letters and key tokens

=================
Vocab initialisation done...

`Training`...for HindiTokenizer


merge 1/50: (224, 164) -> 358 (b'\xe0\xa4') had 5_899_449 occurrences


merge 2/50: (32, 358) -> 359 (b' \xe0\xa4') had 1_869_612 occurrences


merge 3/50: (224, 165) -> 360 (b'\xe0\xa5') had 1_684_235 occurrences


merge 4/50: (358, 190) -> 361 (b'\xe0\xa4\xbe') had 672_066 occurrences


merge 5/50: (360, 135) -> 362 (b'\xe0\xa5\x87') had 573_180 occurrences


merge 6/50: (358, 176) -> 363 (b'\xe0\xa4\xb0') had 429_162 occurrences


merge 7/50: (358, 130) -> 364 (b'\xe0\xa4\x82') had 373_179 occurrences


merge 8/50: (359, 149) -> 365 (b' \xe0\xa4\x95') had 316_576 occurrences


merge 9/50: (360, 128) -> 366 (b'\xe0\xa5\x80') had 314_253 occurrences


merge 10/50: (361, 358) -> 367 (b'\xe0\xa4\xbe\xe0\xa4') had 297_294 occurrences


merge 11/50: (360, 141) -> 368 (b'\xe0\xa5\x8d') had 280_817 occurrences


merge 12/50: (358, 191) -> 369 (b'\xe0\xa4\xbf') had 253_000 occurrences


merge 13/50: (369, 358) -> 370 (b'\xe0\xa4\xbf\xe0\xa4') had 234_708 occurrences


merge 14/50: (360, 139) -> 371 (b'\xe0\xa5\x8b') had 224_209 occurrences


merge 15/50: (368, 358) -> 372 (b'\xe0\xa5\x8d\xe0\xa4') had 217_356 occurrences


merge 16/50: (358, 168) -> 373 (b'\xe0\xa4\xa8') had 183_326 occurrences


merge 17/50: (362, 364) -> 374 (b'\xe0\xa5\x87\xe0\xa4\x82') had 171_145 occurrences


merge 18/50: (358, 149) -> 375 (b'\xe0\xa4\x95') had 155_940 occurrences


merge 19/50: (359, 184) -> 376 (b' \xe0\xa4\xb8') had 153_767 occurrences


merge 20/50: (359, 185) -> 377 (b' \xe0\xa4\xb9') had 144_631 occurrences


merge 21/50: (359, 174) -> 378 (b' \xe0\xa4\xae') had 143_748 occurrences


merge 22/50: (360, 136) -> 379 (b'\xe0\xa5\x88') had 126_998 occurrences


merge 23/50: (358, 184) -> 380 (b'\xe0\xa4\xb8') had 123_101 occurrences


merge 24/50: (358, 164) -> 381 (b'\xe0\xa4\xa4') had 111_313 occurrences


merge 25/50: (359, 172) -> 382 (b' \xe0\xa4\xac') had 110_022 occurrences


merge 26/50: (359, 170) -> 383 (b' \xe0\xa4\xaa') had 106_177 occurrences


merge 27/50: (361, 363) -> 384 (b'\xe0\xa4\xbe\xe0\xa4\xb0') had 101_051 occurrences


merge 28/50: (358, 178) -> 385 (b'\xe0\xa4\xb2') had 100_951 occurrences


merge 29/50: (362, 358) -> 386 (b'\xe0\xa5\x87\xe0\xa4') had 88_961 occurrences


merge 30/50: (358, 185) -> 387 (b'\xe0\xa4\xb9') had 85_393 occurrences


merge 31/50: (358, 174) -> 388 (b'\xe0\xa4\xae') had 84_051 occurrences


merge 32/50: (359, 168) -> 389 (b' \xe0\xa4\xa8') had 81_989 occurrences


merge 33/50: (359, 156) -> 390 (b' \xe0\xa4\x9c') had 78_394 occurrences


merge 34/50: (367, 168) -> 391 (b'\xe0\xa4\xbe\xe0\xa4\xa8') had 76_705 occurrences


merge 35/50: (378, 374) -> 392 (b' \xe0\xa4\xae\xe0\xa5\x87\xe0\xa4\x82') had 76_273 occurrences


merge 36/50: (175, 361) -> 393 (b'\xaf\xe0\xa4\xbe') had 70_650 occurrences


merge 37/50: (377, 379) -> 394 (b' \xe0\xa4\xb9\xe0\xa5\x88') had 68_678 occurrences


merge 38/50: (359, 176) -> 395 (b' \xe0\xa4\xb0') had 66_424 occurrences


merge 39/50: (358, 151) -> 396 (b'\xe0\xa4\x97') had 65_415 occurrences


merge 40/50: (359, 135) -> 397 (b' \xe0\xa4\x87') had 61_584 occurrences


merge 41/50: (360, 129) -> 398 (b'\xe0\xa5\x81') had 61_464 occurrences


merge 42/50: (358, 161) -> 399 (b'\xe0\xa4\xa1') had 60_329 occurrences


merge 43/50: (368, 363) -> 400 (b'\xe0\xa5\x8d\xe0\xa4\xb0') had 59_203 occurrences


merge 44/50: (365, 366) -> 401 (b' \xe0\xa4\x95\xe0\xa5\x80') had 59_058 occurrences


merge 45/50: (359, 178) -> 402 (b' \xe0\xa4\xb2') had 57_987 occurrences


merge 46/50: (358, 170) -> 403 (b'\xe0\xa4\xaa') had 55_198 occurrences


merge 47/50: (358, 166) -> 404 (b'\xe0\xa4\xa6') had 53_445 occurrences


merge 48/50: (359, 164) -> 405 (b' \xe0\xa4\xa4') had 52_864 occurrences


merge 49/50: (359, 166) -> 406 (b' \xe0\xa4\xa6') had 52_525 occurrences


merge 50/50: (376, 362) -> 407 (b' \xe0\xa4\xb8\xe0\xa5\x87') had 48_524 occurrences
Saving tokenizer...


Batch number=====================================================: 1


`Training`...for HindiTokenizer

Pair: (224, 164) already in merged tokens... replacing in IDS...
with.. id.. 358

Pair: (32, 358) already in merged tokens... replacing in IDS...
with.. id.. 359

Pair: (224, 165) already in merged tokens... replacing in IDS...
with.. id.. 360

Pair: (358, 190) already in merged tokens... replacing in IDS...
with.. id.. 361

Pair: (360, 135) already in merged tokens... replacing in IDS...
with.. id.. 362

Pair: (358, 176) already in merged tokens... replacing in IDS...
with.. id.. 363

Pair: (360, 141) already in merged tokens... replacing in IDS...
with.. id.. 368

Pair: (361, 358) already in merged tokens... replacing in IDS...
with.. id.. 367

Pair: (359, 149) already in merged tokens... replacing in IDS...
with.. id.. 365

Pair: (360, 128) already in merged tokens... replacing in IDS...
with.. id.. 366

Pair: (358, 191) already in merged tokens... replacing in IDS...
with.. id.. 369

Pair: (358, 130) already in merged tokens... replacing in IDS...
with.. id.. 364

Pair: (368, 358) already in merged tokens... replacing in IDS...
with.. id.. 372

Pair: (369, 358) already in merged tokens... replacing in IDS...
with.. id.. 370

Pair: (360, 139) already in merged tokens... replacing in IDS...
with.. id.. 371

Pair: (358, 168) already in merged tokens... replacing in IDS...
with.. id.. 373

Pair: (359, 184) already in merged tokens... replacing in IDS...
with.. id.. 376

Pair: (359, 185) already in merged tokens... replacing in IDS...
with.. id.. 377

Pair: (358, 149) already in merged tokens... replacing in IDS...
with.. id.. 375
b'fetch: 33 objects found, done.\nfetch: Fetching all references...\n'
b'Checking out LFS objects: 100% (33/33), 9.6 GB | 0 B/s, done.\n'

=========
Reading dataset
=============

Count of files to read...30


Batch number=====================================================: 0


====================================

Building initial Hindi vocabulary with basic Hindi letters and key tokens

=================
Vocab initialisation done...

`Training`...for HindiTokenizer


merge 1/50: (224, 164) -> 358 (b'\xe0\xa4') had 5_899_449 occurrences


merge 2/50: (32, 358) -> 359 (b' \xe0\xa4') had 1_869_612 occurrences


merge 3/50: (224, 165) -> 360 (b'\xe0\xa5') had 1_684_235 occurrences


merge 4/50: (358, 190) -> 361 (b'\xe0\xa4\xbe') had 672_066 occurrences


merge 5/50: (360, 135) -> 362 (b'\xe0\xa5\x87') had 573_180 occurrences


merge 6/50: (358, 176) -> 363 (b'\xe0\xa4\xb0') had 429_162 occurrences


merge 7/50: (358, 130) -> 364 (b'\xe0\xa4\x82') had 373_179 occurrences


merge 8/50: (359, 149) -> 365 (b' \xe0\xa4\x95') had 316_576 occurrences


merge 9/50: (360, 128) -> 366 (b'\xe0\xa5\x80') had 314_253 occurrences


merge 10/50: (361, 358) -> 367 (b'\xe0\xa4\xbe\xe0\xa4') had 297_294 occurrences


merge 11/50: (360, 141) -> 368 (b'\xe0\xa5\x8d') had 280_817 occurrences


merge 12/50: (358, 191) -> 369 (b'\xe0\xa4\xbf') had 253_000 occurrences


merge 13/50: (369, 358) -> 370 (b'\xe0\xa4\xbf\xe0\xa4') had 234_708 occurrences


merge 14/50: (360, 139) -> 371 (b'\xe0\xa5\x8b') had 224_209 occurrences


merge 15/50: (368, 358) -> 372 (b'\xe0\xa5\x8d\xe0\xa4') had 217_356 occurrences


merge 16/50: (358, 168) -> 373 (b'\xe0\xa4\xa8') had 183_326 occurrences


merge 17/50: (362, 364) -> 374 (b'\xe0\xa5\x87\xe0\xa4\x82') had 171_145 occurrences


merge 18/50: (358, 149) -> 375 (b'\xe0\xa4\x95') had 155_940 occurrences


merge 19/50: (359, 184) -> 376 (b' \xe0\xa4\xb8') had 153_767 occurrences


merge 20/50: (359, 185) -> 377 (b' \xe0\xa4\xb9') had 144_631 occurrences


merge 21/50: (359, 174) -> 378 (b' \xe0\xa4\xae') had 143_748 occurrences


merge 22/50: (360, 136) -> 379 (b'\xe0\xa5\x88') had 126_998 occurrences


merge 23/50: (358, 184) -> 380 (b'\xe0\xa4\xb8') had 123_101 occurrences


merge 24/50: (358, 164) -> 381 (b'\xe0\xa4\xa4') had 111_313 occurrences


merge 25/50: (359, 172) -> 382 (b' \xe0\xa4\xac') had 110_022 occurrences


merge 26/50: (359, 170) -> 383 (b' \xe0\xa4\xaa') had 106_177 occurrences


merge 27/50: (361, 363) -> 384 (b'\xe0\xa4\xbe\xe0\xa4\xb0') had 101_051 occurrences


merge 28/50: (358, 178) -> 385 (b'\xe0\xa4\xb2') had 100_951 occurrences


merge 29/50: (362, 358) -> 386 (b'\xe0\xa5\x87\xe0\xa4') had 88_961 occurrences


merge 30/50: (358, 185) -> 387 (b'\xe0\xa4\xb9') had 85_393 occurrences


merge 31/50: (358, 174) -> 388 (b'\xe0\xa4\xae') had 84_051 occurrences


merge 32/50: (359, 168) -> 389 (b' \xe0\xa4\xa8') had 81_989 occurrences


merge 33/50: (359, 156) -> 390 (b' \xe0\xa4\x9c') had 78_394 occurrences


merge 34/50: (367, 168) -> 391 (b'\xe0\xa4\xbe\xe0\xa4\xa8') had 76_705 occurrences


merge 35/50: (378, 374) -> 392 (b' \xe0\xa4\xae\xe0\xa5\x87\xe0\xa4\x82') had 76_273 occurrences


merge 36/50: (175, 361) -> 393 (b'\xaf\xe0\xa4\xbe') had 70_650 occurrences


merge 37/50: (377, 379) -> 394 (b' \xe0\xa4\xb9\xe0\xa5\x88') had 68_678 occurrences
b'fetch: 33 objects found, done.\nfetch: Fetching all references...\n'
b'Checking out LFS objects: 100% (33/33), 9.6 GB | 0 B/s, done.\n'

=========
Reading dataset
=============

Count of files to read...30


Batch number=====================================================: 0


====================================

Building initial Hindi vocabulary with basic Hindi letters and key tokens

=================
Vocab initialisation done...

`Training`...for HindiTokenizer


merge 1/50: (224, 164) -> 358 (b'\xe0\xa4') had 5_899_449 occurrences


merge 2/50: (32, 358) -> 359 (b' \xe0\xa4') had 1_869_612 occurrences


merge 3/50: (224, 165) -> 360 (b'\xe0\xa5') had 1_684_235 occurrences


merge 4/50: (358, 190) -> 361 (b'\xe0\xa4\xbe') had 672_066 occurrences


merge 5/50: (360, 135) -> 362 (b'\xe0\xa5\x87') had 573_180 occurrences


merge 6/50: (358, 176) -> 363 (b'\xe0\xa4\xb0') had 429_162 occurrences


merge 7/50: (358, 130) -> 364 (b'\xe0\xa4\x82') had 373_179 occurrences


merge 8/50: (359, 149) -> 365 (b' \xe0\xa4\x95') had 316_576 occurrences


merge 9/50: (360, 128) -> 366 (b'\xe0\xa5\x80') had 314_253 occurrences


merge 10/50: (361, 358) -> 367 (b'\xe0\xa4\xbe\xe0\xa4') had 297_294 occurrences


merge 11/50: (360, 141) -> 368 (b'\xe0\xa5\x8d') had 280_817 occurrences


merge 12/50: (358, 191) -> 369 (b'\xe0\xa4\xbf') had 253_000 occurrences


merge 13/50: (369, 358) -> 370 (b'\xe0\xa4\xbf\xe0\xa4') had 234_708 occurrences


merge 14/50: (360, 139) -> 371 (b'\xe0\xa5\x8b') had 224_209 occurrences


merge 15/50: (368, 358) -> 372 (b'\xe0\xa5\x8d\xe0\xa4') had 217_356 occurrences


merge 16/50: (358, 168) -> 373 (b'\xe0\xa4\xa8') had 183_326 occurrences


merge 17/50: (362, 364) -> 374 (b'\xe0\xa5\x87\xe0\xa4\x82') had 171_145 occurrences


merge 18/50: (358, 149) -> 375 (b'\xe0\xa4\x95') had 155_940 occurrences


merge 19/50: (359, 184) -> 376 (b' \xe0\xa4\xb8') had 153_767 occurrences


merge 20/50: (359, 185) -> 377 (b' \xe0\xa4\xb9') had 144_631 occurrences


merge 21/50: (359, 174) -> 378 (b' \xe0\xa4\xae') had 143_748 occurrences


merge 22/50: (360, 136) -> 379 (b'\xe0\xa5\x88') had 126_998 occurrences


merge 23/50: (358, 184) -> 380 (b'\xe0\xa4\xb8') had 123_101 occurrences


merge 24/50: (358, 164) -> 381 (b'\xe0\xa4\xa4') had 111_313 occurrences


merge 25/50: (359, 172) -> 382 (b' \xe0\xa4\xac') had 110_022 occurrences


merge 26/50: (359, 170) -> 383 (b' \xe0\xa4\xaa') had 106_177 occurrences


merge 27/50: (361, 363) -> 384 (b'\xe0\xa4\xbe\xe0\xa4\xb0') had 101_051 occurrences


merge 28/50: (358, 178) -> 385 (b'\xe0\xa4\xb2') had 100_951 occurrences


merge 29/50: (362, 358) -> 386 (b'\xe0\xa5\x87\xe0\xa4') had 88_961 occurrences


merge 30/50: (358, 185) -> 387 (b'\xe0\xa4\xb9') had 85_393 occurrences


merge 31/50: (358, 174) -> 388 (b'\xe0\xa4\xae') had 84_051 occurrences


merge 32/50: (359, 168) -> 389 (b' \xe0\xa4\xa8') had 81_989 occurrences


merge 33/50: (359, 156) -> 390 (b' \xe0\xa4\x9c') had 78_394 occurrences


merge 34/50: (367, 168) -> 391 (b'\xe0\xa4\xbe\xe0\xa4\xa8') had 76_705 occurrences


merge 35/50: (378, 374) -> 392 (b' \xe0\xa4\xae\xe0\xa5\x87\xe0\xa4\x82') had 76_273 occurrences


merge 36/50: (175, 361) -> 393 (b'\xaf\xe0\xa4\xbe') had 70_650 occurrences


merge 37/50: (377, 379) -> 394 (b' \xe0\xa4\xb9\xe0\xa5\x88') had 68_678 occurrences


merge 38/50: (359, 176) -> 395 (b' \xe0\xa4\xb0') had 66_424 occurrences


merge 39/50: (358, 151) -> 396 (b'\xe0\xa4\x97') had 65_415 occurrences


merge 40/50: (359, 135) -> 397 (b' \xe0\xa4\x87') had 61_584 occurrences


merge 41/50: (360, 129) -> 398 (b'\xe0\xa5\x81') had 61_464 occurrences


merge 42/50: (358, 161) -> 399 (b'\xe0\xa4\xa1') had 60_329 occurrences


merge 43/50: (368, 363) -> 400 (b'\xe0\xa5\x8d\xe0\xa4\xb0') had 59_203 occurrences


merge 44/50: (365, 366) -> 401 (b' \xe0\xa4\x95\xe0\xa5\x80') had 59_058 occurrences


merge 45/50: (359, 178) -> 402 (b' \xe0\xa4\xb2') had 57_987 occurrences


merge 46/50: (358, 170) -> 403 (b'\xe0\xa4\xaa') had 55_198 occurrences


merge 47/50: (358, 166) -> 404 (b'\xe0\xa4\xa6') had 53_445 occurrences


merge 48/50: (359, 164) -> 405 (b' \xe0\xa4\xa4') had 52_864 occurrences


merge 49/50: (359, 166) -> 406 (b' \xe0\xa4\xa6') had 52_525 occurrences


merge 50/50: (376, 362) -> 407 (b' \xe0\xa4\xb8\xe0\xa5\x87') had 48_524 occurrences
Saving tokenizer...
b'fetch: 33 objects found, done.\nfetch: Fetching all references...\n'
b'Checking out LFS objects: 100% (33/33), 9.6 GB | 0 B/s, done.\n'

=========
Reading dataset
=============

Count of files to read...30


Batch number=====================================================: 0


====================================

Building initial Hindi vocabulary with basic Hindi letters and key tokens

=================
Vocab initialisation done...

`Training`...for HindiTokenizer


merge 1/50: (224, 164) -> 358 (b'\xe0\xa4') had 5_899_449 occurrences


merge 2/50: (32, 358) -> 359 (b' \xe0\xa4') had 1_869_612 occurrences


merge 3/50: (224, 165) -> 360 (b'\xe0\xa5') had 1_684_235 occurrences


merge 4/50: (358, 190) -> 361 (b'\xe0\xa4\xbe') had 672_066 occurrences


merge 5/50: (360, 135) -> 362 (b'\xe0\xa5\x87') had 573_180 occurrences


merge 6/50: (358, 176) -> 363 (b'\xe0\xa4\xb0') had 429_162 occurrences


merge 7/50: (358, 130) -> 364 (b'\xe0\xa4\x82') had 373_179 occurrences


merge 8/50: (359, 149) -> 365 (b' \xe0\xa4\x95') had 316_576 occurrences


merge 9/50: (360, 128) -> 366 (b'\xe0\xa5\x80') had 314_253 occurrences


merge 10/50: (361, 358) -> 367 (b'\xe0\xa4\xbe\xe0\xa4') had 297_294 occurrences


merge 11/50: (360, 141) -> 368 (b'\xe0\xa5\x8d') had 280_817 occurrences


merge 12/50: (358, 191) -> 369 (b'\xe0\xa4\xbf') had 253_000 occurrences


merge 13/50: (369, 358) -> 370 (b'\xe0\xa4\xbf\xe0\xa4') had 234_708 occurrences


merge 14/50: (360, 139) -> 371 (b'\xe0\xa5\x8b') had 224_209 occurrences


merge 15/50: (368, 358) -> 372 (b'\xe0\xa5\x8d\xe0\xa4') had 217_356 occurrences


merge 16/50: (358, 168) -> 373 (b'\xe0\xa4\xa8') had 183_326 occurrences


merge 17/50: (362, 364) -> 374 (b'\xe0\xa5\x87\xe0\xa4\x82') had 171_145 occurrences


merge 18/50: (358, 149) -> 375 (b'\xe0\xa4\x95') had 155_940 occurrences


merge 19/50: (359, 184) -> 376 (b' \xe0\xa4\xb8') had 153_767 occurrences


merge 20/50: (359, 185) -> 377 (b' \xe0\xa4\xb9') had 144_631 occurrences


merge 21/50: (359, 174) -> 378 (b' \xe0\xa4\xae') had 143_748 occurrences


merge 22/50: (360, 136) -> 379 (b'\xe0\xa5\x88') had 126_998 occurrences


merge 23/50: (358, 184) -> 380 (b'\xe0\xa4\xb8') had 123_101 occurrences


merge 24/50: (358, 164) -> 381 (b'\xe0\xa4\xa4') had 111_313 occurrences


merge 25/50: (359, 172) -> 382 (b' \xe0\xa4\xac') had 110_022 occurrences


merge 26/50: (359, 170) -> 383 (b' \xe0\xa4\xaa') had 106_177 occurrences


merge 27/50: (361, 363) -> 384 (b'\xe0\xa4\xbe\xe0\xa4\xb0') had 101_051 occurrences


merge 28/50: (358, 178) -> 385 (b'\xe0\xa4\xb2') had 100_951 occurrences


merge 29/50: (362, 358) -> 386 (b'\xe0\xa5\x87\xe0\xa4') had 88_961 occurrences


merge 30/50: (358, 185) -> 387 (b'\xe0\xa4\xb9') had 85_393 occurrences


merge 31/50: (358, 174) -> 388 (b'\xe0\xa4\xae') had 84_051 occurrences


merge 32/50: (359, 168) -> 389 (b' \xe0\xa4\xa8') had 81_989 occurrences


merge 33/50: (359, 156) -> 390 (b' \xe0\xa4\x9c') had 78_394 occurrences


merge 34/50: (367, 168) -> 391 (b'\xe0\xa4\xbe\xe0\xa4\xa8') had 76_705 occurrences


merge 35/50: (378, 374) -> 392 (b' \xe0\xa4\xae\xe0\xa5\x87\xe0\xa4\x82') had 76_273 occurrences


merge 36/50: (175, 361) -> 393 (b'\xaf\xe0\xa4\xbe') had 70_650 occurrences


merge 37/50: (377, 379) -> 394 (b' \xe0\xa4\xb9\xe0\xa5\x88') had 68_678 occurrences


merge 38/50: (359, 176) -> 395 (b' \xe0\xa4\xb0') had 66_424 occurrences


merge 39/50: (358, 151) -> 396 (b'\xe0\xa4\x97') had 65_415 occurrences


merge 40/50: (359, 135) -> 397 (b' \xe0\xa4\x87') had 61_584 occurrences


merge 41/50: (360, 129) -> 398 (b'\xe0\xa5\x81') had 61_464 occurrences


merge 42/50: (358, 161) -> 399 (b'\xe0\xa4\xa1') had 60_329 occurrences


merge 43/50: (368, 363) -> 400 (b'\xe0\xa5\x8d\xe0\xa4\xb0') had 59_203 occurrences


merge 44/50: (365, 366) -> 401 (b' \xe0\xa4\x95\xe0\xa5\x80') had 59_058 occurrences


merge 45/50: (359, 178) -> 402 (b' \xe0\xa4\xb2') had 57_987 occurrences


merge 46/50: (358, 170) -> 403 (b'\xe0\xa4\xaa') had 55_198 occurrences


merge 47/50: (358, 166) -> 404 (b'\xe0\xa4\xa6') had 53_445 occurrences


merge 48/50: (359, 164) -> 405 (b' \xe0\xa4\xa4') had 52_864 occurrences


merge 49/50: (359, 166) -> 406 (b' \xe0\xa4\xa6') had 52_525 occurrences


merge 50/50: (376, 362) -> 407 (b' \xe0\xa4\xb8\xe0\xa5\x87') had 48_524 occurrences
Saving tokenizer...
b'fetch: 33 objects found, done.\nfetch: Fetching all references...\n'
b'Checking out LFS objects: 100% (33/33), 9.6 GB | 0 B/s, done.\n'

=========
Reading dataset
=============

Count of files to read...30


Batch number=====================================================: 0


====================================

Building initial Hindi vocabulary with basic Hindi letters and key tokens

=================
Vocab initialisation done...

`Training`...for HindiTokenizer


merge 1/50: (224, 164) -> 358 (b'\xe0\xa4') had 5_899_449 occurrences


merge 2/50: (32, 358) -> 359 (b' \xe0\xa4') had 1_869_612 occurrences


merge 3/50: (224, 165) -> 360 (b'\xe0\xa5') had 1_684_235 occurrences


merge 4/50: (358, 190) -> 361 (b'\xe0\xa4\xbe') had 672_066 occurrences


merge 5/50: (360, 135) -> 362 (b'\xe0\xa5\x87') had 573_180 occurrences


merge 6/50: (358, 176) -> 363 (b'\xe0\xa4\xb0') had 429_162 occurrences


merge 7/50: (358, 130) -> 364 (b'\xe0\xa4\x82') had 373_179 occurrences


merge 8/50: (359, 149) -> 365 (b' \xe0\xa4\x95') had 316_576 occurrences


merge 9/50: (360, 128) -> 366 (b'\xe0\xa5\x80') had 314_253 occurrences


merge 10/50: (361, 358) -> 367 (b'\xe0\xa4\xbe\xe0\xa4') had 297_294 occurrences


merge 11/50: (360, 141) -> 368 (b'\xe0\xa5\x8d') had 280_817 occurrences


merge 12/50: (358, 191) -> 369 (b'\xe0\xa4\xbf') had 253_000 occurrences


merge 13/50: (369, 358) -> 370 (b'\xe0\xa4\xbf\xe0\xa4') had 234_708 occurrences


merge 14/50: (360, 139) -> 371 (b'\xe0\xa5\x8b') had 224_209 occurrences


merge 15/50: (368, 358) -> 372 (b'\xe0\xa5\x8d\xe0\xa4') had 217_356 occurrences


merge 16/50: (358, 168) -> 373 (b'\xe0\xa4\xa8') had 183_326 occurrences


merge 17/50: (362, 364) -> 374 (b'\xe0\xa5\x87\xe0\xa4\x82') had 171_145 occurrences


merge 18/50: (358, 149) -> 375 (b'\xe0\xa4\x95') had 155_940 occurrences


merge 19/50: (359, 184) -> 376 (b' \xe0\xa4\xb8') had 153_767 occurrences


merge 20/50: (359, 185) -> 377 (b' \xe0\xa4\xb9') had 144_631 occurrences


merge 21/50: (359, 174) -> 378 (b' \xe0\xa4\xae') had 143_748 occurrences


merge 22/50: (360, 136) -> 379 (b'\xe0\xa5\x88') had 126_998 occurrences


merge 23/50: (358, 184) -> 380 (b'\xe0\xa4\xb8') had 123_101 occurrences


merge 24/50: (358, 164) -> 381 (b'\xe0\xa4\xa4') had 111_313 occurrences


merge 25/50: (359, 172) -> 382 (b' \xe0\xa4\xac') had 110_022 occurrences


merge 26/50: (359, 170) -> 383 (b' \xe0\xa4\xaa') had 106_177 occurrences


merge 27/50: (361, 363) -> 384 (b'\xe0\xa4\xbe\xe0\xa4\xb0') had 101_051 occurrences


merge 28/50: (358, 178) -> 385 (b'\xe0\xa4\xb2') had 100_951 occurrences


merge 29/50: (362, 358) -> 386 (b'\xe0\xa5\x87\xe0\xa4') had 88_961 occurrences


merge 30/50: (358, 185) -> 387 (b'\xe0\xa4\xb9') had 85_393 occurrences


merge 31/50: (358, 174) -> 388 (b'\xe0\xa4\xae') had 84_051 occurrences


merge 32/50: (359, 168) -> 389 (b' \xe0\xa4\xa8') had 81_989 occurrences


merge 33/50: (359, 156) -> 390 (b' \xe0\xa4\x9c') had 78_394 occurrences


merge 34/50: (367, 168) -> 391 (b'\xe0\xa4\xbe\xe0\xa4\xa8') had 76_705 occurrences


merge 35/50: (378, 374) -> 392 (b' \xe0\xa4\xae\xe0\xa5\x87\xe0\xa4\x82') had 76_273 occurrences


merge 36/50: (175, 361) -> 393 (b'\xaf\xe0\xa4\xbe') had 70_650 occurrences


merge 37/50: (377, 379) -> 394 (b' \xe0\xa4\xb9\xe0\xa5\x88') had 68_678 occurrences


merge 38/50: (359, 176) -> 395 (b' \xe0\xa4\xb0') had 66_424 occurrences


merge 39/50: (358, 151) -> 396 (b'\xe0\xa4\x97') had 65_415 occurrences


merge 40/50: (359, 135) -> 397 (b' \xe0\xa4\x87') had 61_584 occurrences


merge 41/50: (360, 129) -> 398 (b'\xe0\xa5\x81') had 61_464 occurrences


merge 42/50: (358, 161) -> 399 (b'\xe0\xa4\xa1') had 60_329 occurrences


merge 43/50: (368, 363) -> 400 (b'\xe0\xa5\x8d\xe0\xa4\xb0') had 59_203 occurrences


merge 44/50: (365, 366) -> 401 (b' \xe0\xa4\x95\xe0\xa5\x80') had 59_058 occurrences


merge 45/50: (359, 178) -> 402 (b' \xe0\xa4\xb2') had 57_987 occurrences


merge 46/50: (358, 170) -> 403 (b'\xe0\xa4\xaa') had 55_198 occurrences


merge 47/50: (358, 166) -> 404 (b'\xe0\xa4\xa6') had 53_445 occurrences


merge 48/50: (359, 164) -> 405 (b' \xe0\xa4\xa4') had 52_864 occurrences


merge 49/50: (359, 166) -> 406 (b' \xe0\xa4\xa6') had 52_525 occurrences


merge 50/50: (376, 362) -> 407 (b' \xe0\xa4\xb8\xe0\xa5\x87') had 48_524 occurrences
Saving tokenizer...


Batch number=====================================================: 1


`Training`...for HindiTokenizer

Pair: (224, 164) already in merged tokens... replacing in IDS...
with.. id.. 358

Pair: (32, 358) already in merged tokens... replacing in IDS...
with.. id.. 359
b'fetch: 33 objects found, done.\nfetch: Fetching all references...\n'
b'Checking out LFS objects: 100% (33/33), 9.6 GB | 0 B/s, done.\n'

=========
Reading dataset
=============

Count of files to read...30


Batch number=====================================================: 0


====================================

Building initial Hindi vocabulary with basic Hindi letters and key tokens

=================
Vocab initialisation done...

`Training`...for HindiTokenizer


merge 1/50: (224, 164) -> 358 (b'\xe0\xa4') had 5_899_449 occurrences


merge 2/50: (32, 358) -> 359 (b' \xe0\xa4') had 1_869_612 occurrences


merge 3/50: (224, 165) -> 360 (b'\xe0\xa5') had 1_684_235 occurrences


merge 4/50: (358, 190) -> 361 (b'\xe0\xa4\xbe') had 672_066 occurrences


merge 5/50: (360, 135) -> 362 (b'\xe0\xa5\x87') had 573_180 occurrences
b'fetch: 33 objects found, done.\nfetch: Fetching all references...\n'
b'Checking out LFS objects: 100% (33/33), 9.6 GB | 0 B/s, done.\n'

=========
Reading dataset
=============

Count of files to read...30


Batch number=====================================================: 0


====================================

Building initial Hindi vocabulary with basic Hindi letters and key tokens

=================
Vocab initialisation done...

`Training`...for HindiTokenizer
b'fetch: 33 objects found, done.\nfetch: Fetching all references...\n'
b'Checking out LFS objects: 100% (33/33), 9.6 GB | 0 B/s, done.\n'

=========
Reading dataset
=============

Count of files to read...30


Batch number=====================================================: 0


====================================

Building initial Hindi vocabulary with basic Hindi letters and key tokens

=================
Vocab initialisation done...

`Training`...for HindiTokenizer


merge 1/10: (224, 164) -> 358 (b'\xe0\xa4') had 5_899_449 occurrences


merge 2/10: (32, 358) -> 359 (b' \xe0\xa4') had 1_869_612 occurrences


merge 3/10: (224, 165) -> 360 (b'\xe0\xa5') had 1_684_235 occurrences


merge 4/10: (358, 190) -> 361 (b'\xe0\xa4\xbe') had 672_066 occurrences


merge 5/10: (360, 135) -> 362 (b'\xe0\xa5\x87') had 573_180 occurrences


merge 6/10: (358, 176) -> 363 (b'\xe0\xa4\xb0') had 429_162 occurrences


merge 7/10: (358, 130) -> 364 (b'\xe0\xa4\x82') had 373_179 occurrences


merge 8/10: (359, 149) -> 365 (b' \xe0\xa4\x95') had 316_576 occurrences


merge 9/10: (360, 128) -> 366 (b'\xe0\xa5\x80') had 314_253 occurrences


merge 10/10: (361, 358) -> 367 (b'\xe0\xa4\xbe\xe0\xa4') had 297_294 occurrences
Saving tokenizer...


Batch number=====================================================: 1


`Training`...for HindiTokenizer

Pair: (224, 164) already in merged tokens... replacing in IDS...
with.. id.. 358

Pair: (32, 358) already in merged tokens... replacing in IDS...
with.. id.. 359

Pair: (224, 165) already in merged tokens... replacing in IDS...
with.. id.. 360

Pair: (358, 190) already in merged tokens... replacing in IDS...
with.. id.. 361

Pair: (360, 135) already in merged tokens... replacing in IDS...
with.. id.. 362

Pair: (358, 176) already in merged tokens... replacing in IDS...
with.. id.. 363


merge 1/10: (360, 141) -> 368 (b'\xe0\xa5\x8d') had 528_294 occurrences

Pair: (361, 358) already in merged tokens... replacing in IDS...
with.. id.. 367

Pair: (359, 149) already in merged tokens... replacing in IDS...
with.. id.. 365

Pair: (360, 128) already in merged tokens... replacing in IDS...
with.. id.. 366


merge 2/10: (358, 191) -> 369 (b'\xe0\xa4\xbf') had 457_640 occurrences

Pair: (358, 130) already in merged tokens... replacing in IDS...
with.. id.. 364


merge 3/10: (368, 358) -> 370 (b'\xe0\xa5\x8d\xe0\xa4') had 396_202 occurrences


merge 4/10: (369, 358) -> 371 (b'\xe0\xa4\xbf\xe0\xa4') had 360_982 occurrences


merge 5/10: (360, 139) -> 372 (b'\xe0\xa5\x8b') had 326_668 occurrences


merge 6/10: (358, 168) -> 373 (b'\xe0\xa4\xa8') had 275_129 occurrences


merge 7/10: (359, 184) -> 374 (b' \xe0\xa4\xb8') had 241_765 occurrences


merge 8/10: (359, 185) -> 375 (b' \xe0\xa4\xb9') had 238_160 occurrences


merge 9/10: (358, 149) -> 376 (b'\xe0\xa4\x95') had 234_822 occurrences


merge 10/10: (358, 164) -> 377 (b'\xe0\xa4\xa4') had 207_475 occurrences
Saving tokenizer...


Batch number=====================================================: 2


`Training`...for HindiTokenizer

Pair: (224, 164) already in merged tokens... replacing in IDS...
with.. id.. 358

Pair: (32, 358) already in merged tokens... replacing in IDS...
with.. id.. 359

Pair: (224, 165) already in merged tokens... replacing in IDS...
with.. id.. 360

Pair: (358, 190) already in merged tokens... replacing in IDS...
with.. id.. 361
b'fetch: 33 objects found, done.\nfetch: Fetching all references...\n'
b'fetch: 33 objects found, done.\nfetch: Fetching all references...\n'
b'Checking out LFS objects: 100% (33/33), 9.6 GB | 0 B/s, done.\n'

=========
Reading dataset
=============

Count of files to read...30


Batch number=====================================================: 0


====================================

Building initial Hindi vocabulary with basic Hindi letters and key tokens

=================
Vocab initialisation done...

`Training`...for HindiTokenizer


merge 1/10: (224, 164) -> 358 (b'\xe0\xa4') had 5_899_449 occurrences


merge 2/10: (32, 358) -> 359 (b' \xe0\xa4') had 1_869_612 occurrences


merge 3/10: (224, 165) -> 360 (b'\xe0\xa5') had 1_684_235 occurrences


merge 4/10: (358, 190) -> 361 (b'\xe0\xa4\xbe') had 672_066 occurrences


merge 5/10: (360, 135) -> 362 (b'\xe0\xa5\x87') had 573_180 occurrences


merge 6/10: (358, 176) -> 363 (b'\xe0\xa4\xb0') had 429_162 occurrences


merge 7/10: (358, 130) -> 364 (b'\xe0\xa4\x82') had 373_179 occurrences


merge 8/10: (359, 149) -> 365 (b' \xe0\xa4\x95') had 316_576 occurrences


merge 9/10: (360, 128) -> 366 (b'\xe0\xa5\x80') had 314_253 occurrences


merge 10/10: (361, 358) -> 367 (b'\xe0\xa4\xbe\xe0\xa4') had 297_294 occurrences
Saving tokenizer...


Batch number=====================================================: 1


`Training`...for HindiTokenizer

Pair: (224, 164) already in merged tokens... replacing in IDS...
with.. id.. 358

Pair: (32, 358) already in merged tokens... replacing in IDS...
with.. id.. 359

Pair: (224, 165) already in merged tokens... replacing in IDS...
with.. id.. 360

Pair: (358, 190) already in merged tokens... replacing in IDS...
with.. id.. 361

Pair: (360, 135) already in merged tokens... replacing in IDS...
with.. id.. 362

Pair: (358, 176) already in merged tokens... replacing in IDS...
with.. id.. 363


merge 1/10: (360, 141) -> 368 (b'\xe0\xa5\x8d') had 528_294 occurrences

Pair: (361, 358) already in merged tokens... replacing in IDS...
with.. id.. 367

Pair: (359, 149) already in merged tokens... replacing in IDS...
with.. id.. 365

Pair: (360, 128) already in merged tokens... replacing in IDS...
with.. id.. 366


merge 2/10: (358, 191) -> 369 (b'\xe0\xa4\xbf') had 457_640 occurrences

Pair: (358, 130) already in merged tokens... replacing in IDS...
with.. id.. 364


merge 3/10: (368, 358) -> 370 (b'\xe0\xa5\x8d\xe0\xa4') had 396_202 occurrences


merge 4/10: (369, 358) -> 371 (b'\xe0\xa4\xbf\xe0\xa4') had 360_982 occurrences


merge 5/10: (360, 139) -> 372 (b'\xe0\xa5\x8b') had 326_668 occurrences


merge 6/10: (358, 168) -> 373 (b'\xe0\xa4\xa8') had 275_129 occurrences


merge 7/10: (359, 184) -> 374 (b' \xe0\xa4\xb8') had 241_765 occurrences


merge 8/10: (359, 185) -> 375 (b' \xe0\xa4\xb9') had 238_160 occurrences


merge 9/10: (358, 149) -> 376 (b'\xe0\xa4\x95') had 234_822 occurrences


merge 10/10: (358, 164) -> 377 (b'\xe0\xa4\xa4') had 207_475 occurrences
Saving tokenizer...


Batch number=====================================================: 2


`Training`...for HindiTokenizer

Pair: (224, 164) already in merged tokens... replacing in IDS...
with.. id.. 358

Pair: (32, 358) already in merged tokens... replacing in IDS...
with.. id.. 359

Pair: (224, 165) already in merged tokens... replacing in IDS...
with.. id.. 360

Pair: (358, 190) already in merged tokens... replacing in IDS...
with.. id.. 361

Pair: (360, 135) already in merged tokens... replacing in IDS...
with.. id.. 362

Pair: (358, 176) already in merged tokens... replacing in IDS...
with.. id.. 363

Pair: (360, 141) already in merged tokens... replacing in IDS...
with.. id.. 368
b'fetch: 33 objects found, done.\nfetch: Fetching all references...\n'
b'Checking out LFS objects: 100% (33/33), 9.6 GB | 0 B/s, done.\n'

=========
Reading dataset
=============

Count of files to read...30


Batch number=====================================================: 0


====================================

Building initial Hindi vocabulary with basic Hindi letters and key tokens

=================
Vocab initialisation done...

`Training`...for HindiTokenizer


merge 1/10: (224, 164) -> 358 (b'\xe0\xa4') had 5_899_449 occurrences


merge 2/10: (32, 358) -> 359 (b' \xe0\xa4') had 1_869_612 occurrences


merge 3/10: (224, 165) -> 360 (b'\xe0\xa5') had 1_684_235 occurrences


merge 4/10: (358, 190) -> 361 (b'\xe0\xa4\xbe') had 672_066 occurrences


merge 5/10: (360, 135) -> 362 (b'\xe0\xa5\x87') had 573_180 occurrences


merge 6/10: (358, 176) -> 363 (b'\xe0\xa4\xb0') had 429_162 occurrences


merge 7/10: (358, 130) -> 364 (b'\xe0\xa4\x82') had 373_179 occurrences


merge 8/10: (359, 149) -> 365 (b' \xe0\xa4\x95') had 316_576 occurrences


merge 9/10: (360, 128) -> 366 (b'\xe0\xa5\x80') had 314_253 occurrences


merge 10/10: (361, 358) -> 367 (b'\xe0\xa4\xbe\xe0\xa4') had 297_294 occurrences
b'fetch: 33 objects found, done.\nfetch: Fetching all references...\n'
b'Checking out LFS objects: 100% (33/33), 9.6 GB | 0 B/s, done.\n'

=========
Reading dataset
=============

Count of files to read...30


Batch number=====================================================: 0


====================================

Building initial Hindi vocabulary with basic Hindi letters and key tokens

=================
Vocab initialisation done...

`Training`...for HindiTokenizer


merge 1/10: (224, 164) -> 358 (b'\xe0\xa4') had 5_899_449 occurrences


merge 2/10: (32, 358) -> 359 (b' \xe0\xa4') had 1_869_612 occurrences


merge 3/10: (224, 165) -> 360 (b'\xe0\xa5') had 1_684_235 occurrences


merge 4/10: (358, 190) -> 361 (b'\xe0\xa4\xbe') had 672_066 occurrences


merge 5/10: (360, 135) -> 362 (b'\xe0\xa5\x87') had 573_180 occurrences


merge 6/10: (358, 176) -> 363 (b'\xe0\xa4\xb0') had 429_162 occurrences


merge 7/10: (358, 130) -> 364 (b'\xe0\xa4\x82') had 373_179 occurrences


merge 8/10: (359, 149) -> 365 (b' \xe0\xa4\x95') had 316_576 occurrences


merge 9/10: (360, 128) -> 366 (b'\xe0\xa5\x80') had 314_253 occurrences


merge 10/10: (361, 358) -> 367 (b'\xe0\xa4\xbe\xe0\xa4') had 297_294 occurrences
Saving tokenizer...


Batch number=====================================================: 1


`Training`...for HindiTokenizer

Pair: (224, 164) already in merged tokens... replacing in IDS...
with.. id.. 358

Pair: (32, 358) already in merged tokens... replacing in IDS...
with.. id.. 359

Pair: (224, 165) already in merged tokens... replacing in IDS...
with.. id.. 360

Pair: (358, 190) already in merged tokens... replacing in IDS...
with.. id.. 361

Pair: (360, 135) already in merged tokens... replacing in IDS...
with.. id.. 362

Pair: (358, 176) already in merged tokens... replacing in IDS...
with.. id.. 363


merge 1/10: (360, 141) -> 368 (b'\xe0\xa5\x8d') had 528_294 occurrences

Pair: (361, 358) already in merged tokens... replacing in IDS...
with.. id.. 367

Pair: (359, 149) already in merged tokens... replacing in IDS...
with.. id.. 365

Pair: (360, 128) already in merged tokens... replacing in IDS...
with.. id.. 366


merge 2/10: (358, 191) -> 369 (b'\xe0\xa4\xbf') had 457_640 occurrences

Pair: (358, 130) already in merged tokens... replacing in IDS...
with.. id.. 364


merge 3/10: (368, 358) -> 370 (b'\xe0\xa5\x8d\xe0\xa4') had 396_202 occurrences


merge 4/10: (369, 358) -> 371 (b'\xe0\xa4\xbf\xe0\xa4') had 360_982 occurrences


merge 5/10: (360, 139) -> 372 (b'\xe0\xa5\x8b') had 326_668 occurrences


merge 6/10: (358, 168) -> 373 (b'\xe0\xa4\xa8') had 275_129 occurrences


merge 7/10: (359, 184) -> 374 (b' \xe0\xa4\xb8') had 241_765 occurrences


merge 8/10: (359, 185) -> 375 (b' \xe0\xa4\xb9') had 238_160 occurrences


merge 9/10: (358, 149) -> 376 (b'\xe0\xa4\x95') had 234_822 occurrences


merge 10/10: (358, 164) -> 377 (b'\xe0\xa4\xa4') had 207_475 occurrences
Saving tokenizer...


Batch number=====================================================: 2


`Training`...for HindiTokenizer

Pair: (224, 164) already in merged tokens... replacing in IDS...
with.. id.. 358

Pair: (32, 358) already in merged tokens... replacing in IDS...
with.. id.. 359

Pair: (224, 165) already in merged tokens... replacing in IDS...
with.. id.. 360
b'fetch: 33 objects found, done.\nfetch: Fetching all references...\n'
b'Checking out LFS objects: 100% (33/33), 9.6 GB | 0 B/s, done.\n'

=========
Reading dataset
=============

Count of files to read...30


Batch number=====================================================: 0


====================================

Building initial Hindi vocabulary with basic Hindi letters and key tokens

=================
Vocab initialisation done...

`Training`...for HindiTokenizer


merge 1/10: (224, 164) -> 358 (b'\xe0\xa4') had 5_899_449 occurrences


merge 2/10: (32, 358) -> 359 (b' \xe0\xa4') had 1_869_612 occurrences


merge 3/10: (224, 165) -> 360 (b'\xe0\xa5') had 1_684_235 occurrences


merge 4/10: (358, 190) -> 361 (b'\xe0\xa4\xbe') had 672_066 occurrences
